{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the target function\n",
    "def target_function(x):\n",
    "    \"\"\"f(x) = x*(1-x)*sin(2Ï€/(x+0.1))\"\"\"\n",
    "    return x * (1 - x) * torch.sin((2 * np.pi) / (x + 0.1))\n",
    "\n",
    "# Your improved ChebyKANLayer\n",
    "class ChebyKANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, degree):\n",
    "        super(ChebyKANLayer, self).__init__()\n",
    "        self.inputdim = input_dim\n",
    "        self.outdim = output_dim\n",
    "        self.degree = degree\n",
    "\n",
    "        self.cheby_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n",
    "        nn.init.normal_(self.cheby_coeffs, mean=0.0, std=1/(input_dim*(degree+1)))\n",
    "        self.register_buffer(\"arange\", torch.arange(0, degree + 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize to [-1, 1] using tanh\n",
    "        x = torch.tanh(x)\n",
    "        # View and repeat input degree + 1 times\n",
    "        x = x.view((-1, self.inputdim, 1)).expand(-1, -1, self.degree + 1)\n",
    "        # Apply acos and multiply by arange [0..degree]\n",
    "        x = (x.acos() * self.arange).cos()\n",
    "        # Compute Chebyshev interpolation using einsum\n",
    "        y = torch.einsum(\"bid,iod->bo\", x, self.cheby_coeffs)\n",
    "        return y.view(-1, self.outdim)\n",
    "\n",
    "# Multi-layer ChebyKAN model\n",
    "class ChebyKAN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=10, output_dim=1, degree=8):\n",
    "        super(ChebyKAN, self).__init__()\n",
    "        self.chebykan1 = ChebyKANLayer(input_dim, hidden_dim, degree)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.chebykan2 = ChebyKANLayer(hidden_dim, hidden_dim, degree)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.chebykan3 = ChebyKANLayer(hidden_dim, output_dim, degree)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.chebykan1(x)\n",
    "        x = torch.tanh(self.ln1(x))  # Additional non-linearity\n",
    "        x = self.chebykan2(x)\n",
    "        x = torch.tanh(self.ln2(x))  # Additional non-linearity\n",
    "        x = self.chebykan3(x)\n",
    "        return x\n",
    "\n",
    "# Create dataset\n",
    "def create_dataset(num_samples=1000):\n",
    "    x = torch.rand(num_samples, 1)  # Input in [0, 1]\n",
    "    y = target_function(x)  # Target values\n",
    "    return x, y\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ChebyKAN(input_dim=1, hidden_dim=16, output_dim=1, degree=12)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5)\n",
    "\n",
    "# Create data\n",
    "X_train, y_train = create_dataset(5000)\n",
    "X_test = torch.linspace(0, 1, 1000).unsqueeze(1)\n",
    "y_test = target_function(X_test)\n",
    "\n",
    "# Move data to device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "batch_size = 64\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"Training improved ChebyKAN on complex function...\")\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Mini-batch training\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = X_train[indices]\n",
    "        batch_y = y_train[indices]\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(batch_x)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = epoch_loss / (X_train.size(0) / batch_size)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = criterion(test_pred, y_test).item()\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        tqdm.write(f'Epoch {epoch:4d}/{num_epochs}, Train Loss: {avg_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_predictions = model(X_test)\n",
    "    final_loss = criterion(final_predictions, y_test).item()\n",
    "\n",
    "print(f\"\\nFinal Test Loss: {final_loss:.8f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Function approximation\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(X_test.cpu().numpy(), y_test.cpu().numpy(), 'b-', label='True Function', linewidth=2, alpha=0.8)\n",
    "plt.plot(X_test.cpu().numpy(), final_predictions.cpu().numpy(), 'r--', label='ChebyKAN Prediction', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'Function Approximation (Loss: {final_loss:.2e})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Training and test loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Absolute error\n",
    "plt.subplot(1, 3, 3)\n",
    "abs_error = torch.abs(final_predictions - y_test).cpu().numpy()\n",
    "plt.plot(X_test.cpu().numpy(), abs_error, 'g-', label='Absolute Error')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('|Error|')\n",
    "plt.title('Approximation Error')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show model architecture and parameters\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
